{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db30c18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# df = pd.read_csv('deceptive-opinion.csv')\n",
    "\n",
    "\n",
    "# import nltk\n",
    "# from nltk.stem import PorterStemmer\n",
    "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Conv1D, MaxPooling1D, Dropout, Flatten, concatenate\n",
    "\n",
    "# import re\n",
    "# import string \n",
    "# def text_cleaning(text):\n",
    "#     '''\n",
    "#         Make text lowercase, remove text in square brackets, remove link, remove special characters, \n",
    "#         and remove words containing numbers.\n",
    "#     '''\n",
    "    \n",
    "#     text = text.lower()\n",
    "#     text = re.sub('\\[.*?\\]','',text)\n",
    "#     text = re.sub('\\\\W',\" \", text)\n",
    "#     text = re.sub('https?://\\S+|www\\.\\S+','',text)\n",
    "#     text = re.sub('<.*?>+','',text)\n",
    "#     text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "#     text = re.sub('\\n', '', text)\n",
    "#     text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    \n",
    "#     return text\n",
    "\n",
    "# df['text']=df['text'].apply(text_cleaning)\n",
    "\n",
    "\n",
    "# # Load and preprocess the data\n",
    "# reviews = df['text'].values # Load your dataset of reviews\n",
    "# labels = df['deceptive'].values# Load the corresponding labels (0 for deceptive, 1 for truthful)\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# labels = le.fit_transform(labels)\n",
    "\n",
    "# # x_train, x_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.2, random_state=1000)\n",
    "# # x_train, x_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.2, random_state=0)\n",
    "# # x_train, x_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.2, random_state=69)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.2, random_state=143)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# tokenizer = Tokenizer(num_words=30000)\n",
    "# tokenizer.fit_on_texts(x_train)\n",
    "# sequences = tokenizer.texts_to_sequences(x_train)\n",
    "# x_train = pad_sequences(sequences, maxlen=100, padding = 'post')\n",
    "\n",
    "# sequences_test = tokenizer.texts_to_sequences(x_test)\n",
    "# x_test = pad_sequences(sequences_test, maxlen=100, padding = 'post')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Define the generator network\n",
    "# generator = tf.keras.Sequential([\n",
    "#     layers.Embedding(20000, 128,input_shape=(100,),input_length=100),\n",
    "# #     layers.Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "# #     layers.MaxPooling1D(pool_size=3),\n",
    "#     layers.LSTM(500, return_sequences=True),\n",
    "\n",
    "#     layers.LSTM(500),\n",
    "#     layers.Dense(100, activation='sigmoid')\n",
    "\n",
    "# #     layers.Dense(10000,activation='relu'),\n",
    "# #     layers.Dense(10000,activation='relu'),\n",
    "# #     layers.Dense(10000, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "# # Define the discriminator network\n",
    "# discriminator = tf.keras.Sequential([\n",
    "#     layers.Embedding(20000, 128,input_shape=(100,),input_length=100),\n",
    "#     layers.Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "#     layers.MaxPooling1D(pool_size=3),\n",
    "#     layers.LSTM(500),\n",
    "#     layers.Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "    \n",
    "    \n",
    "    \n",
    "# #     layers.Dense(1000, input_shape=(10000,), activation='relu'),\n",
    "# #     layers.Dense(1000, activation='relu'),\n",
    "# #     layers.Dense(1000, activation='relu'),\n",
    "# #     layers.Dense(1000, activation='relu'),\n",
    "# #     layers.Dense(1000, activation='relu'),\n",
    "# #     layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "\n",
    "# # Define the GAN\n",
    "# gan = tf.keras.Sequential([\n",
    "#     generator,\n",
    "#     discriminator,\n",
    "# #     discriminator\n",
    "# ])\n",
    "\n",
    "# # Compile the GAN\n",
    "# discriminator.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "# discriminator.trainable = False\n",
    "# gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# # Train the GAN\n",
    "# for epoch in range(100):\n",
    "#     # Train the discriminator on real reviews\n",
    "#     indices = tf.range(len(x_test))\n",
    "#     tf.random.shuffle(indices)\n",
    "# #     real_reviews = x_train[indices[:32]]\n",
    "# #     real_labels = tf.ones((32,))\n",
    "#     history_test = discriminator.train_on_batch(x_test, y_test)\n",
    "\n",
    "#     # Train the discriminator on fake reviews\n",
    "# #     100 because of maxlen=100 for embedding\n",
    "# #     noise = tf.random.normal((32, 100))\n",
    "#     noise = tf.random.uniform((70, 100), minval=0, maxval=20000, dtype=tf.int32)\n",
    "\n",
    "#     fake_reviews = generator(noise)\n",
    "#     fake_labels = tf.zeros((70,))\n",
    "#     history_fake = discriminator.train_on_batch(fake_reviews, fake_labels)\n",
    "\n",
    "#     # Train the generator\n",
    "# #     generator_loss = gan.train_on_batch(fake_reviews, fake_labels)\n",
    "# #     print(generator_loss)\n",
    "\n",
    "# # Use the discriminator to classify new reviews\n",
    "# def classify_review(review):\n",
    "#     sequence = tokenizer.texts_to_sequences([review])\n",
    "#     print(sequence)\n",
    "#     x = pad_sequences(sequence, maxlen=100)\n",
    "#     print(x)\n",
    "#     prob = np.round(discriminator.predict(x)[0][0])\n",
    "#     print(prob)\n",
    "    \n",
    "#     if prob > 0.5:\n",
    "#         return 'truthful'\n",
    "#         print(\"truthful\")\n",
    "\n",
    "#     else:\n",
    "#         return 'deceptive'\n",
    "#         print(\"deceptive\")\n",
    "\n",
    "        \n",
    "# new_text = \"I had to do a little research on the Hilton brand because it's not one that I was familiar with. I wanted to know what this hotel chain's reputation was, so I did a little research online and found out that it's pretty well-known as a chain that people book into for their business trips and vacations. I also found out that this hotel chain is pretty well-reviewed by people who have stayed in their hotels.\"\n",
    "# y_pred = (gan.predict(x_test) > 0.5).astype(int).flatten()\n",
    "# accuracy = np.mean(y_pred == y_test)\n",
    "# print('Test accuracy:', accuracy)\n",
    "\n",
    "\n",
    "# classify_review(new_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc78c94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 22:18:25.901529: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-21 22:18:38.016281: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 4, 1)         149121      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " discriminator (Discriminator)  (None, 1)            25089       ['sequential[0][0]']             \n",
      "                                                                                                  \n",
      " discriminator_1 (Discriminator  (None, 1)           25089       ['sequential[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 199,299\n",
      "Trainable params: 149,121\n",
      "Non-trainable params: 50,178\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('deceptive-opinion.csv')\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Conv1D, MaxPooling1D, Dropout, Flatten, concatenate\n",
    "\n",
    "import re\n",
    "import string \n",
    "def text_cleaning(text):\n",
    "    '''\n",
    "        Make text lowercase, remove text in square brackets, remove link, remove special characters, \n",
    "        and remove words containing numbers.\n",
    "    '''\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]','',text)\n",
    "    text = re.sub('\\\\W',\" \", text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+','',text)\n",
    "    text = re.sub('<.*?>+','',text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "df['text']=df['text'].apply(text_cleaning)\n",
    "\n",
    "\n",
    "# Load and preprocess the data\n",
    "reviews = df['text'].values # Load your dataset of reviews\n",
    "labels = df['deceptive'].values# Load the corresponding labels (0 for deceptive, 1 for truthful)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.2, random_state=1000)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.2, random_state=0)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.2, random_state=69)\n",
    "x_train, x_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.2, random_state=143)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=30000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "x_train = pad_sequences(sequences, maxlen=100, padding = 'post')\n",
    "\n",
    "sequences_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_test = pad_sequences(sequences_test, maxlen=100, padding = 'post')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the generator\n",
    "generator = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(256, input_shape=(100,), activation='relu'),\n",
    "    tf.keras.layers.Reshape((1, 256)),\n",
    "    tf.keras.layers.Conv1DTranspose(128, 3, strides=2, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Conv1DTranspose(64, 3, strides=2, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Conv1DTranspose(1, 3, strides=1, padding='same', activation='tanh'),\n",
    "])\n",
    "\n",
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv1D(64, 3, strides=2, padding='same', input_shape=(None, 1))\n",
    "        self.leaky_relu1 = tf.keras.layers.LeakyReLU()\n",
    "        self.dropout1 = tf.keras.layers.Dropout(0.3)\n",
    "        self.conv2 = tf.keras.layers.Conv1D(128, 3, strides=2, padding='same')\n",
    "        self.leaky_relu2 = tf.keras.layers.LeakyReLU()\n",
    "        self.dropout2 = tf.keras.layers.Dropout(0.3)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.leaky_relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.leaky_relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        return x\n",
    "\n",
    "discriminator_1 = Discriminator()\n",
    "discriminator_2 = Discriminator()\n",
    "\n",
    "\n",
    "# # Define the two discriminators\n",
    "# discriminator_1 = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Conv1D(64, 3, strides=2, padding='same', input_shape=(None, 1)),\n",
    "#     tf.keras.layers.LeakyReLU(),\n",
    "#     tf.keras.layers.Dropout(0.3),\n",
    "#     tf.keras.layers.Conv1D(128, 3, strides=2, padding='same'),\n",
    "#     tf.keras.layers.LeakyReLU(),\n",
    "#     tf.keras.layers.Dropout(0.3),\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "# discriminator_2 = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Conv1D(64, 3, strides=2, padding='same', input_shape=(None, 1)),\n",
    "#     tf.keras.layers.LeakyReLU(),\n",
    "#     tf.keras.layers.Dropout(0.3),\n",
    "#     tf.keras.layers.Conv1D(128, 3, strides=2, padding='same'),\n",
    "#     tf.keras.layers.LeakyReLU(),\n",
    "#     tf.keras.layers.Dropout(0.3),\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "# Define the combined model\n",
    "latent_dim = 100\n",
    "gan_input = tf.keras.layers.Input(shape=(latent_dim,))\n",
    "generated_sequence = generator(gan_input)\n",
    "discriminator_1_output = discriminator_1(generated_sequence)\n",
    "discriminator_2_output = discriminator_2(generated_sequence)\n",
    "gan = tf.keras.models.Model(gan_input, [discriminator_1_output, discriminator_2_output])\n",
    "\n",
    "# Compile the discriminators separately\n",
    "discriminator_1.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5), metrics=['accuracy'])\n",
    "discriminator_2.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5), metrics=['accuracy'])\n",
    "\n",
    "# Make the discriminator trainable in the combined model\n",
    "discriminator_1.trainable = False\n",
    "discriminator_2.trainable = False\n",
    "\n",
    "# Compile the combined model\n",
    "gan.compile(loss=['binary_crossentropy', 'binary_crossentropy'], optimizer=tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5))\n",
    "\n",
    "# Print the model summary\n",
    "gan.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d45112ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/Users/deth/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/deth/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/deth/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/deth/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/deth/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/folders/rs/9j57x0q16d9722p1wf50s4vw0000gn/T/__autograph_generated_fileo1jca_0_.py\", line 10, in tf__call\n        x = ag__.converted_call(ag__.ld(self).conv1, (ag__.ld(x),), None, fscope)\n\n    TypeError: Exception encountered when calling layer 'discriminator' (type Discriminator).\n    \n    in user code:\n    \n        File \"/var/folders/rs/9j57x0q16d9722p1wf50s4vw0000gn/T/ipykernel_1468/557183559.py\", line 102, in call  *\n            x = self.conv1(x)\n        File \"/Users/deth/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        TypeError: Exception encountered when calling layer 'conv1d' (type Conv1D).\n        \n        Input 'filter' of 'Conv2D' Op has type float32 that does not match type int32 of argument 'input'.\n        \n        Call arguments received by layer 'conv1d' (type Conv1D):\n          • inputs=tf.Tensor(shape=(128, 100, 1), dtype=int32)\n    \n    \n    Call arguments received by layer 'discriminator' (type Discriminator):\n      • x=tf.Tensor(shape=(128, 100, 1), dtype=int32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rs/9j57x0q16d9722p1wf50s4vw0000gn/T/ipykernel_1468/414155956.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Train the discriminators on the real and fake sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mdiscriminator_1_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_seq_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mdiscriminator_1_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_seq_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mdiscriminator_2_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_seq_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   2476\u001b[0m             )\n\u001b[1;32m   2477\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2478\u001b[0;31m             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mstep_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1231\u001b[0m                 )\n\u001b[1;32m   1232\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m             outputs = reduce_per_replica(\n\u001b[1;32m   1235\u001b[0m                 \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m                 \u001b[0;31m# Ensure counter is updated only if `train_step` succeeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_minimum_control_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[0;31m# Run forward pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_target_and_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/rs/9j57x0q16d9722p1wf50s4vw0000gn/T/__autograph_generated_fileo1jca_0_.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/Users/deth/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/deth/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/deth/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/deth/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/deth/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/folders/rs/9j57x0q16d9722p1wf50s4vw0000gn/T/__autograph_generated_fileo1jca_0_.py\", line 10, in tf__call\n        x = ag__.converted_call(ag__.ld(self).conv1, (ag__.ld(x),), None, fscope)\n\n    TypeError: Exception encountered when calling layer 'discriminator' (type Discriminator).\n    \n    in user code:\n    \n        File \"/var/folders/rs/9j57x0q16d9722p1wf50s4vw0000gn/T/ipykernel_1468/557183559.py\", line 102, in call  *\n            x = self.conv1(x)\n        File \"/Users/deth/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        TypeError: Exception encountered when calling layer 'conv1d' (type Conv1D).\n        \n        Input 'filter' of 'Conv2D' Op has type float32 that does not match type int32 of argument 'input'.\n        \n        Call arguments received by layer 'conv1d' (type Conv1D):\n          • inputs=tf.Tensor(shape=(128, 100, 1), dtype=int32)\n    \n    \n    Call arguments received by layer 'discriminator' (type Discriminator):\n      • x=tf.Tensor(shape=(128, 100, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the batch size and number of epochs\n",
    "batch_size = 128\n",
    "epochs = 10000\n",
    "\n",
    "# Generate some random noise for the generator\n",
    "def generate_noise(batch_size, latent_dim):\n",
    "    return np.random.normal(0, 1, size=(batch_size, latent_dim))\n",
    "\n",
    "# Define the number of batches per epoch\n",
    "steps_per_epoch = int(10000 / batch_size)\n",
    "\n",
    "# Train the model for the specified number of epochs\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Train the discriminator on real and fake sequences\n",
    "    for step in range(steps_per_epoch):\n",
    "        seq_length = x_train.shape[1]\n",
    "\n",
    "        # Get a batch of real sequences\n",
    "        idx = np.random.randint(0, x_test.shape[0], batch_size)\n",
    "\n",
    "        real_seq_batch = x_test[idx].reshape(batch_size, seq_length, 1)\n",
    "        # Generate a batch of fake sequences\n",
    "        noise = generate_noise(batch_size, latent_dim)\n",
    "        fake_seq_batch = generator.predict(noise)\n",
    "\n",
    "        # Train the discriminators on the real and fake sequences\n",
    "        discriminator_1_loss_real = discriminator_1.train_on_batch(real_seq_batch, np.ones(batch_size))\n",
    "        discriminator_1_loss_fake = discriminator_1.train_on_batch(fake_seq_batch, np.zeros(batch_size))\n",
    "        discriminator_2_loss_real = discriminator_2.train_on_batch(real_seq_batch, np.ones(batch_size))\n",
    "        discriminator_2_loss_fake = discriminator_2.train_on_batch(fake_seq_batch, np.zeros(batch_size))\n",
    "        discriminator_loss = 0.5 * np.add.reduce([discriminator_1_loss_real, discriminator_1_loss_fake,\n",
    "                                                  discriminator_2_loss_real, discriminator_2_loss_fake])\n",
    "\n",
    "    # Train the generator to fool the discriminators\n",
    "    noise = generate_noise(batch_size, latent_dim)\n",
    "    gan_loss = gan.train_on_batch(noise, [np.ones(batch_size), np.ones(batch_size)])\n",
    "\n",
    "    # Print the loss every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch %d/%d - discriminator loss: %f, generator loss: %f' % (epoch+1, epochs, discriminator_loss, gan_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe9f772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a batch of noise and pass it through the generator\n",
    "noise = generate_noise(batch_size, latent_dim)\n",
    "generated_sequences = generator.predict(noise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1066d88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5dcb58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2846e6e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10a7536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64381d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef77364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
